{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ff95c31-bc75-4f45-83ef-64555993d8ca",
   "metadata": {
    "id": "eBKhfH6gZO1A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import pandas_datareader\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime, timedelta\n",
    "from prettytable import PrettyTable\n",
    "import cvxpy as cvx\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import requests\n",
    "import json\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, losses\n",
    "from sklearn.svm import SVR\n",
    "import statsmodels.api as sm\n",
    "import scipy.optimize as opt\n",
    "from scipy.cluster.hierarchy import linkage, cut_tree, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "from fredapi import Fred\n",
    "import getFamaFrenchFactors as gff\n",
    "import quantstats as qs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85ed0a-9c97-4312-9e50-93522393b8bc",
   "metadata": {
    "id": "L2SLXBVimUrL",
    "tags": []
   },
   "source": [
    "# Describing initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b977e1a-7aa0-4c4e-8301-c7002bc8fd48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Jks_jxrYj8A",
    "outputId": "f2f0427e-7b21-4651-d59f-0f39e182c74c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date: 2020-01-02\n",
      "End Date: 2021-01-01\n"
     ]
    }
   ],
   "source": [
    "#Total years of traing model\n",
    "years_training=1\n",
    "# Define the end date\n",
    "fecha = \"2021-01-01\"\n",
    "end=fecha\n",
    "# Convert the end date string to a datetime object\n",
    "end_date = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "# Calculate the start date as one year before the end date\n",
    "start_date = end_date - timedelta(days=365*years_training)\n",
    "# Convert the start date to a string in the same format as the end date\n",
    "start = start_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(\"Start Date:\", start)\n",
    "print(\"End Date:\", end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1947ec4b-b9ee-484b-9a14-203fd1c30872",
   "metadata": {
    "id": "YTMnUQFdabAM"
   },
   "outputs": [],
   "source": [
    "## define the stocks for our portfolio (via tickers) ###\n",
    "assets = [ #Healthcare\n",
    "          \"JNJ\", #(Johnson & Johnson)\n",
    "          \"PFE\", #(Pfizer Inc.)\n",
    "          \"MRK\", #(Merck & Co. Inc.)\n",
    "          \"LLY\", #(Eli Lilly and company)\n",
    "          \"UNH\", #(UnitedHealth Group Incorporated)\n",
    "          \"AMGN\", #(Amgen Inc.)\n",
    "          #Technology\n",
    "          \"MSFT\", #(Microsoft Corporation)\n",
    "          \"GOOGL\", #(Alphabet Inc. - Google)\n",
    "          \"META\", #(Meta Platforms Inc. - Facebook)\n",
    "          \"NVDA\", #(Nvidia)\n",
    "          \"AMD\", #(Advanced micro devices)\n",
    "          \"TSLA\", #(Tesla, Inc.)\n",
    "          \"CRM\", #(Salesforce.com Inc.)\n",
    "          # Finance\n",
    "          \"JPM\", #(JPMorgan Chase & Co.)\n",
    "          \"MA\", #(Mastercard)\n",
    "          \"BX\", #(Blackstone)\n",
    "          \"AXP\", #(American Express)\n",
    "          \"GS\", #(The Goldman Sachs Group, Inc.)\n",
    "          \"BAC\", #(Bank of America Corporation)\n",
    "          \"V\", #(Visa Inc.)\n",
    "          #Consumer goods\n",
    "          \"PG\", #(Procter & Gamble Company)\n",
    "          \"KO\", #(The Coca-Cola Company)\n",
    "          \"NKE\", #(Nike, Inc.)\n",
    "          \"MCD\", #(McDonald's Corporation)\n",
    "          #Energy\n",
    "          \"XOM\", #(Exxon Mobil Corporation)\n",
    "          \"CVX\", #(Chevron Corporation)\n",
    "          \"SHEL\", #(Royal Dutch Shell plc)\n",
    "          \"BP\", #(BP p.l.c.)\n",
    "          \"COP\", #(ConocoPhillips)\n",
    "          #Retail\n",
    "          \"AMZN\", #(Amazon.com, Inc.)\n",
    "          \"WMT\", #(Walmart Inc.)\n",
    "          \"TGT\", #(Target Corporation)\n",
    "          \"COST\", #(Costco Wholesale Corporation)\n",
    "          \"HD\", #(The Home Depot, Inc.)\n",
    "          #Telecommunications\n",
    "          \"T\", #(AT&T Inc.)\n",
    "          \"VZ\", #(Verizon Communications Inc.)\n",
    "          \"TMUS\", #(T-Mobile US, Inc.)\n",
    "          #Utilities\n",
    "          \"NEE\", #(NextEra Energy, Inc.)\n",
    "          \"DUK\", #(Duke Energy Corporation)\n",
    "          \"SO\", #(The Southern Company)\n",
    "          \"NRG\", #(NRG Energy)\n",
    "          \"EXC\", #(Exelon Corporation)\n",
    "          \"AEP\", #(American Electric Power Company, Inc.)\n",
    "          #Real Estate\n",
    "          \"SPG\", #(Simon Property Group, Inc.)\n",
    "          \"AMT\", #(American Tower Corporation)\n",
    "          \"EQIX\", #(Equinix, Inc.)\n",
    "          \"PSA\", #(Public Storage)\n",
    "          \"WELL\", #(Welltower Inc.)\n",
    "          #Transportation\n",
    "          \"UPS\", #(United Parcel Service, Inc.)\n",
    "          \"FDX\", #(FedEx Corporation)\n",
    "          \"DAL\", #(Delta Air Lines, Inc.)\n",
    "          \"AAL\", #(American Airlines Group Inc.)\n",
    "          \"BLDR\", #(Builders first source)\n",
    "          #\"UBER\", #(Uber Technologies, Inc.)                  No data before may 2018\n",
    "          #\"ROAD\", #(Construction Partners, Inc)               No data before may 2018\n",
    "          #cripto\n",
    "          \"BTC-USD\", #Bitcoin\n",
    "          \"ETH-USD\" #Ethereum\n",
    "          ]\n",
    "\n",
    "assets =sorted(assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bab971-1247-4e1c-8033-0dbda7ffa7d8",
   "metadata": {
    "id": "0CC5HwyXS72j"
   },
   "source": [
    "## Micro and macroeconomic factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707195d6-ce9d-4216-a3a7-8f7b1825814f",
   "metadata": {
    "id": "z13pbBqza1zI"
   },
   "source": [
    "### FRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d0d2e69-97c7-4d0c-bbf3-237ef5fc21f3",
   "metadata": {
    "id": "T3Ppu3XHawdX"
   },
   "outputs": [],
   "source": [
    "# Initialize Fred with your API key\n",
    "api_key = '95c774645924b13f93458e248a922d67'\n",
    "fred = Fred(api_key=api_key)\n",
    "# Fetch GDP data (quarterly)\n",
    "gdp = fred.get_series('GDP')\n",
    "\n",
    "# Fetch Inflation (Consumer Price Index for All Urban Consumers)\n",
    "inflation = fred.get_series('CPIAUCSL')\n",
    "\n",
    "# Fetch Unemployment rate (monthly)\n",
    "unemployment = fred.get_series('UNRATE')\n",
    "\n",
    "# Fetch Federal Funds Rate (interest rate, monthly)\n",
    "interest_rates = fred.get_series('FEDFUNDS')\n",
    "\n",
    "# Combine the data into a DataFrame\n",
    "macroeconomic_data = pd.concat([gdp, inflation, unemployment, interest_rates], axis=1)\n",
    "macroeconomic_data.columns = ['GDP', 'Inflation', 'Unemployment', 'Interest Rates']\n",
    "\n",
    "# Industrial Production Index (a proxy for industry-specific data)\n",
    "industrial_production = fred.get_series('INDPRO')\n",
    "\n",
    "# Average Hourly Earnings of All Employees, Total Private\n",
    "hourly_earnings = fred.get_series('CES0500000003')\n",
    "\n",
    "# Housing starts (a measure of consumer sentiment in the housing market)\n",
    "housing_starts = fred.get_series('HOUST')\n",
    "\n",
    "# Combine with other microeconomic data\n",
    "microeconomic_data = pd.concat([industrial_production, hourly_earnings, housing_starts], axis=1)\n",
    "microeconomic_data.columns = ['Industrial Production','Average Hourly Earnings',  'Housing Starts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8576ddf2-4ca7-404d-9c82-91e906552d4a",
   "metadata": {
    "id": "ezZUlJunbDDb"
   },
   "outputs": [],
   "source": [
    "# Combine macro and microeconomic data\n",
    "combined_data = pd.concat([macroeconomic_data, microeconomic_data], axis=1)\n",
    "\n",
    "# Align the data to a common frequency (e.g., monthly)\n",
    "combined_data = combined_data.resample('M').ffill()  # Forward-fill to avoid NaN gaps\n",
    "\n",
    "#Get data only from the start date\n",
    "combined_data= combined_data[combined_data.index > start]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "978ca763-04ce-49a5-b5ae-b9edeec8aa2e",
   "metadata": {
    "id": "C6w_VfUkUYw2"
   },
   "outputs": [],
   "source": [
    "# Resample to daily frequency\n",
    "macro_daily = combined_data.resample('D').asfreq()\n",
    "\n",
    "# Forward-fill NaN values for daily frequency\n",
    "macro_daily.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f148d5-9a84-4327-a28b-2aded60bc051",
   "metadata": {
    "id": "XydYMDf5WOhh"
   },
   "source": [
    "#### Daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b6063c3-8ea3-4cf4-8429-09166b1512e9",
   "metadata": {
    "id": "HZKtI2EsWNRQ"
   },
   "outputs": [],
   "source": [
    "# U.S. Dollars to Euro Spot Exchange Rate daily\n",
    "dollar_euro = fred.get_series('DEXUSEU')\n",
    "\n",
    "# Market Yield on U.S. Treasury Securities at 10-Year Constant Maturity, Quoted on an Investment Basis\n",
    "ten_Y = fred.get_series('DEXUSEU')\n",
    "\n",
    "# Dow Jones Industrial Average\n",
    "Dow = fred.get_series('DJIA')\n",
    "\n",
    "# S&P 500\n",
    "sp500 = fred.get_series('SP500')\n",
    "\n",
    "# NASDAQ Composite Index\n",
    "nasdaq = fred.get_series('NASDAQCOM')\n",
    "\n",
    "# CBOE Gold ETF Volatility Index\n",
    "gold = fred.get_series('GVZCLS')\n",
    "\n",
    "# CBOE Crude Oil ETF Volatility Index\n",
    "oil = fred.get_series('OVXCLS')\n",
    "\n",
    "# CBOE Russell 2000 Volatility Index\n",
    "russell = fred.get_series('RVXCLS')\n",
    "\n",
    "# Combine daily data\n",
    "daily_data = pd.concat([dollar_euro, ten_Y, Dow,  sp500, nasdaq, gold, oil, russell], axis=1)\n",
    "daily_data.columns = ['U.S. Dollars to Euro', 'Treasury Securities at 10-Year', 'Dow Jones','S&P 500', 'NASDAQ', 'Gold', 'Crude oil', 'Russell 2000']\n",
    "#Get data only from the start date\n",
    "daily_data= daily_data[daily_data.index > start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d62ea62e-4a8a-4449-82f8-0066ffdc9754",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "id": "MZT79cf8egXB",
    "outputId": "baf84d7b-8511-4dd7-b4f8-04f31be94b32"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDP</th>\n",
       "      <th>Inflation</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Interest Rates</th>\n",
       "      <th>Industrial Production</th>\n",
       "      <th>Average Hourly Earnings</th>\n",
       "      <th>Housing Starts</th>\n",
       "      <th>U.S. Dollars to Euro</th>\n",
       "      <th>Treasury Securities at 10-Year</th>\n",
       "      <th>Dow Jones</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>NASDAQ</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Crude oil</th>\n",
       "      <th>Russell 2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-31</th>\n",
       "      <td>21706.513</td>\n",
       "      <td>258.906</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.55</td>\n",
       "      <td>101.3768</td>\n",
       "      <td>28.44</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>1.1082</td>\n",
       "      <td>1.1082</td>\n",
       "      <td>28256.03</td>\n",
       "      <td>3225.52</td>\n",
       "      <td>9150.94</td>\n",
       "      <td>13.50</td>\n",
       "      <td>40.35</td>\n",
       "      <td>20.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-03</th>\n",
       "      <td>21706.513</td>\n",
       "      <td>258.906</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.55</td>\n",
       "      <td>101.3768</td>\n",
       "      <td>28.44</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>1.1062</td>\n",
       "      <td>1.1062</td>\n",
       "      <td>28399.81</td>\n",
       "      <td>3248.92</td>\n",
       "      <td>9273.40</td>\n",
       "      <td>12.48</td>\n",
       "      <td>41.81</td>\n",
       "      <td>19.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-04</th>\n",
       "      <td>21706.513</td>\n",
       "      <td>258.906</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.55</td>\n",
       "      <td>101.3768</td>\n",
       "      <td>28.44</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>1.1043</td>\n",
       "      <td>1.1043</td>\n",
       "      <td>28807.63</td>\n",
       "      <td>3297.59</td>\n",
       "      <td>9467.97</td>\n",
       "      <td>12.01</td>\n",
       "      <td>39.44</td>\n",
       "      <td>17.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-05</th>\n",
       "      <td>21706.513</td>\n",
       "      <td>258.906</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.55</td>\n",
       "      <td>101.3768</td>\n",
       "      <td>28.44</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>1.1006</td>\n",
       "      <td>1.1006</td>\n",
       "      <td>29290.85</td>\n",
       "      <td>3334.69</td>\n",
       "      <td>9508.68</td>\n",
       "      <td>11.32</td>\n",
       "      <td>37.88</td>\n",
       "      <td>16.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-06</th>\n",
       "      <td>21706.513</td>\n",
       "      <td>258.906</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.55</td>\n",
       "      <td>101.3768</td>\n",
       "      <td>28.44</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>1.0979</td>\n",
       "      <td>1.0979</td>\n",
       "      <td>29379.77</td>\n",
       "      <td>3345.78</td>\n",
       "      <td>9572.15</td>\n",
       "      <td>11.45</td>\n",
       "      <td>36.44</td>\n",
       "      <td>16.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-24</th>\n",
       "      <td>28284.498</td>\n",
       "      <td>312.230</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.33</td>\n",
       "      <td>102.6577</td>\n",
       "      <td>34.68</td>\n",
       "      <td>1321.0</td>\n",
       "      <td>1.0687</td>\n",
       "      <td>1.0687</td>\n",
       "      <td>38460.92</td>\n",
       "      <td>5071.63</td>\n",
       "      <td>15712.75</td>\n",
       "      <td>17.45</td>\n",
       "      <td>29.04</td>\n",
       "      <td>22.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-25</th>\n",
       "      <td>28284.498</td>\n",
       "      <td>312.230</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.33</td>\n",
       "      <td>102.6577</td>\n",
       "      <td>34.68</td>\n",
       "      <td>1321.0</td>\n",
       "      <td>1.0721</td>\n",
       "      <td>1.0721</td>\n",
       "      <td>38085.80</td>\n",
       "      <td>5048.42</td>\n",
       "      <td>15611.76</td>\n",
       "      <td>17.32</td>\n",
       "      <td>28.34</td>\n",
       "      <td>22.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-26</th>\n",
       "      <td>28284.498</td>\n",
       "      <td>312.230</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.33</td>\n",
       "      <td>102.6577</td>\n",
       "      <td>34.68</td>\n",
       "      <td>1321.0</td>\n",
       "      <td>1.0686</td>\n",
       "      <td>1.0686</td>\n",
       "      <td>38239.66</td>\n",
       "      <td>5099.96</td>\n",
       "      <td>15927.90</td>\n",
       "      <td>16.92</td>\n",
       "      <td>27.81</td>\n",
       "      <td>21.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-29</th>\n",
       "      <td>28284.498</td>\n",
       "      <td>312.230</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.33</td>\n",
       "      <td>102.6577</td>\n",
       "      <td>34.68</td>\n",
       "      <td>1321.0</td>\n",
       "      <td>1.0717</td>\n",
       "      <td>1.0717</td>\n",
       "      <td>38386.09</td>\n",
       "      <td>5116.17</td>\n",
       "      <td>15983.08</td>\n",
       "      <td>16.92</td>\n",
       "      <td>27.84</td>\n",
       "      <td>20.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-30</th>\n",
       "      <td>28284.498</td>\n",
       "      <td>312.230</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.33</td>\n",
       "      <td>102.6577</td>\n",
       "      <td>34.75</td>\n",
       "      <td>1321.0</td>\n",
       "      <td>1.0684</td>\n",
       "      <td>1.0684</td>\n",
       "      <td>37815.92</td>\n",
       "      <td>5035.69</td>\n",
       "      <td>15657.82</td>\n",
       "      <td>16.54</td>\n",
       "      <td>27.87</td>\n",
       "      <td>21.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1057 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  GDP  Inflation  Unemployment  Interest Rates  \\\n",
       "2020-01-31  21706.513    258.906           3.6            1.55   \n",
       "2020-02-03  21706.513    258.906           3.6            1.55   \n",
       "2020-02-04  21706.513    258.906           3.6            1.55   \n",
       "2020-02-05  21706.513    258.906           3.6            1.55   \n",
       "2020-02-06  21706.513    258.906           3.6            1.55   \n",
       "...               ...        ...           ...             ...   \n",
       "2024-04-24  28284.498    312.230           3.8            5.33   \n",
       "2024-04-25  28284.498    312.230           3.8            5.33   \n",
       "2024-04-26  28284.498    312.230           3.8            5.33   \n",
       "2024-04-29  28284.498    312.230           3.8            5.33   \n",
       "2024-04-30  28284.498    312.230           3.9            5.33   \n",
       "\n",
       "            Industrial Production  Average Hourly Earnings  Housing Starts  \\\n",
       "2020-01-31               101.3768                    28.44          1572.0   \n",
       "2020-02-03               101.3768                    28.44          1572.0   \n",
       "2020-02-04               101.3768                    28.44          1572.0   \n",
       "2020-02-05               101.3768                    28.44          1572.0   \n",
       "2020-02-06               101.3768                    28.44          1572.0   \n",
       "...                           ...                      ...             ...   \n",
       "2024-04-24               102.6577                    34.68          1321.0   \n",
       "2024-04-25               102.6577                    34.68          1321.0   \n",
       "2024-04-26               102.6577                    34.68          1321.0   \n",
       "2024-04-29               102.6577                    34.68          1321.0   \n",
       "2024-04-30               102.6577                    34.75          1321.0   \n",
       "\n",
       "            U.S. Dollars to Euro  Treasury Securities at 10-Year  Dow Jones  \\\n",
       "2020-01-31                1.1082                          1.1082   28256.03   \n",
       "2020-02-03                1.1062                          1.1062   28399.81   \n",
       "2020-02-04                1.1043                          1.1043   28807.63   \n",
       "2020-02-05                1.1006                          1.1006   29290.85   \n",
       "2020-02-06                1.0979                          1.0979   29379.77   \n",
       "...                          ...                             ...        ...   \n",
       "2024-04-24                1.0687                          1.0687   38460.92   \n",
       "2024-04-25                1.0721                          1.0721   38085.80   \n",
       "2024-04-26                1.0686                          1.0686   38239.66   \n",
       "2024-04-29                1.0717                          1.0717   38386.09   \n",
       "2024-04-30                1.0684                          1.0684   37815.92   \n",
       "\n",
       "            S&P 500    NASDAQ   Gold  Crude oil  Russell 2000  \n",
       "2020-01-31  3225.52   9150.94  13.50      40.35         20.33  \n",
       "2020-02-03  3248.92   9273.40  12.48      41.81         19.78  \n",
       "2020-02-04  3297.59   9467.97  12.01      39.44         17.54  \n",
       "2020-02-05  3334.69   9508.68  11.32      37.88         16.41  \n",
       "2020-02-06  3345.78   9572.15  11.45      36.44         16.43  \n",
       "...             ...       ...    ...        ...           ...  \n",
       "2024-04-24  5071.63  15712.75  17.45      29.04         22.34  \n",
       "2024-04-25  5048.42  15611.76  17.32      28.34         22.16  \n",
       "2024-04-26  5099.96  15927.90  16.92      27.81         21.04  \n",
       "2024-04-29  5116.17  15983.08  16.92      27.84         20.63  \n",
       "2024-04-30  5035.69  15657.82  16.54      27.87         21.48  \n",
       "\n",
       "[1057 rows x 15 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine macro and daily data\n",
    "macro= pd.concat([macro_daily, daily_data], axis=1).dropna()\n",
    "macro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127c405-dc4f-4578-bebc-e47ee0971863",
   "metadata": {
    "id": "DQ7NgtKzhFrr"
   },
   "source": [
    "### EIA api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05a87c8e-c951-44a6-b3e6-6168e3a872a0",
   "metadata": {
    "id": "UsRkeCsWhJdQ"
   },
   "outputs": [],
   "source": [
    "api_url = 'https://api.eia.gov/v2/steo/data'\n",
    "\n",
    "params = {\"api_key\": \"whsAp9dwvJg3dXf6Dx1y4BlbFji8hzZExBuZaL4g\"}\n",
    "\n",
    "header = {\n",
    "    \"frequency\": \"monthly\",\n",
    "    \"data\": [\n",
    "        \"value\"\n",
    "    ],\n",
    "    \"facets\": {\n",
    "        \"seriesId\": [\n",
    "            \"COPR_OPEC\", #Global crude oil production\n",
    "            \"PAPR_WORLD\",\n",
    "            'BREPUSS',\n",
    "            'COPR_OPEC',\n",
    "            'D2TCPUS',\n",
    "            'PASXPUS',\n",
    "            'RAIMUUS',\n",
    "            'T3_STCHANGE_US'\n",
    "        ]\n",
    "    },\n",
    "    \"start\": start,\n",
    "    \"end\": \"2024-04\",\n",
    "    \"sort\": [\n",
    "        {\n",
    "            \"column\": \"period\",\n",
    "            \"direction\": \"desc\"\n",
    "        }\n",
    "    ],\n",
    "    \"offset\": 0,\n",
    "    \"length\": 5000\n",
    "}\n",
    "\n",
    "r = requests.get(api_url, params=params, headers={\"X-Params\": json.dumps(header)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a0efc29-bdb2-465d-a674-6f6b21690346",
   "metadata": {
    "id": "feS5po9shPeH"
   },
   "outputs": [],
   "source": [
    "# Transform data to a dataframe\n",
    "file = r.json()\n",
    "eia=pd.DataFrame(file['response']['data'])\n",
    "# Set the date as a index\n",
    "eia['period']=pd.to_datetime(eia['period'])\n",
    "eia.set_index('period', inplace=True)\n",
    "# Transform the data to have all the macroeconomic data in columns\n",
    "eai_macro = eia.pivot_table(index='period', columns='seriesId', values='value', aggfunc='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a730c5db-31c9-4ac4-88e3-5db833ccfe05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "QWvKdjF9hmE8",
    "outputId": "058b5a2f-cc97-4d7e-9a4e-47fe3c195460"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>seriesId</th>\n",
       "      <th>COPR_OPEC</th>\n",
       "      <th>PAPR_WORLD</th>\n",
       "      <th>PASXPUS</th>\n",
       "      <th>RAIMUUS</th>\n",
       "      <th>T3_STCHANGE_US</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>27.32</td>\n",
       "      <td>101.00411098</td>\n",
       "      <td>1299.893185</td>\n",
       "      <td>53.87</td>\n",
       "      <td>-.58108270968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>27.32</td>\n",
       "      <td>101.00411098</td>\n",
       "      <td>1299.893185</td>\n",
       "      <td>53.87</td>\n",
       "      <td>-.58108270968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>27.32</td>\n",
       "      <td>101.00411098</td>\n",
       "      <td>1299.893185</td>\n",
       "      <td>53.87</td>\n",
       "      <td>-.58108270968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <td>27.32</td>\n",
       "      <td>101.00411098</td>\n",
       "      <td>1299.893185</td>\n",
       "      <td>53.87</td>\n",
       "      <td>-.58108270968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <td>27.32</td>\n",
       "      <td>101.00411098</td>\n",
       "      <td>1299.893185</td>\n",
       "      <td>53.87</td>\n",
       "      <td>-.58108270968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-28</th>\n",
       "      <td>26.895</td>\n",
       "      <td>102.53086846</td>\n",
       "      <td>1212.4401753</td>\n",
       "      <td>78.53</td>\n",
       "      <td>.20367834654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-29</th>\n",
       "      <td>26.895</td>\n",
       "      <td>102.53086846</td>\n",
       "      <td>1212.4401753</td>\n",
       "      <td>78.53</td>\n",
       "      <td>.20367834654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-30</th>\n",
       "      <td>26.895</td>\n",
       "      <td>102.53086846</td>\n",
       "      <td>1212.4401753</td>\n",
       "      <td>78.53</td>\n",
       "      <td>.20367834654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-31</th>\n",
       "      <td>26.895</td>\n",
       "      <td>102.53086846</td>\n",
       "      <td>1212.4401753</td>\n",
       "      <td>78.53</td>\n",
       "      <td>.20367834654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-01</th>\n",
       "      <td>26.87</td>\n",
       "      <td>102.08300495</td>\n",
       "      <td>1230.3266895</td>\n",
       "      <td>82.6</td>\n",
       "      <td>-.70027259672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1553 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "seriesId   COPR_OPEC    PAPR_WORLD       PASXPUS RAIMUUS T3_STCHANGE_US\n",
       "period                                                                 \n",
       "2020-01-01     27.32  101.00411098   1299.893185   53.87  -.58108270968\n",
       "2020-01-02     27.32  101.00411098   1299.893185   53.87  -.58108270968\n",
       "2020-01-03     27.32  101.00411098   1299.893185   53.87  -.58108270968\n",
       "2020-01-04     27.32  101.00411098   1299.893185   53.87  -.58108270968\n",
       "2020-01-05     27.32  101.00411098   1299.893185   53.87  -.58108270968\n",
       "...              ...           ...           ...     ...            ...\n",
       "2024-03-28    26.895  102.53086846  1212.4401753   78.53   .20367834654\n",
       "2024-03-29    26.895  102.53086846  1212.4401753   78.53   .20367834654\n",
       "2024-03-30    26.895  102.53086846  1212.4401753   78.53   .20367834654\n",
       "2024-03-31    26.895  102.53086846  1212.4401753   78.53   .20367834654\n",
       "2024-04-01     26.87  102.08300495  1230.3266895    82.6  -.70027259672\n",
       "\n",
       "[1553 rows x 5 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resample to daily frequency\n",
    "eai_daily = eai_macro.resample('D').asfreq()\n",
    "\n",
    "# Forward-fill NaN values for daily frequency\n",
    "eai_daily.fillna(method='ffill', inplace=True)\n",
    "eai_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbd22849-d7e8-480d-b400-d74d2d4bcd54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sqrc70okWUI3",
    "outputId": "256ffa7f-bb01-47ad-c80f-5af6ff40c5b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  55 of 55 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-02 2021-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assets =sorted(assets)\n",
    "df_prices = yf.download(assets,start=start,end=end)['Adj Close']\n",
    "df_prices_all = df_prices.dropna()\n",
    "market_prices_all = yf.download(\"^GSPC\",start=start,end=end)['Adj Close']\n",
    "print(start, end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a1a1c-fcd2-4352-a070-dadd0b1c43a7",
   "metadata": {
    "id": "jMSWn0062yRL"
   },
   "source": [
    "If there is singularity problem like \"LinAlgError: SVD did not converge\" Check the values of the assets. Could have nan value because there not exist in yahoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "417b5645-3e14-4612-bfcb-28a9d7c725fe",
   "metadata": {
    "id": "L0FUt4jxZuhm"
   },
   "outputs": [],
   "source": [
    "# Join the micro and macroeconomic with all the asset to use it as input in the machine learning\n",
    "macro_prices = df_prices_all.join(macro, how='inner')  # Join the Fred economic variable with the assets value\n",
    "df = macro_prices.join(eai_daily, how='inner') # Join the EAI economic variable with the preview table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b40afbd-e302-4c4e-a469-53d895fbbd5e",
   "metadata": {},
   "source": [
    "# Evaluation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5d38a85-074d-46e8-b6f7-f2cd9ae74836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "leng_period= 60\n",
    "#Choose only the assets as output\n",
    "returns= df[assets]\n",
    "returns = returns.pct_change(periods=leng_period).shift(-leng_period)\n",
    "# Split into training and test sets (without using `train_test_split`)\n",
    "# For training and testing we are using the data leng_periods before because the target will be the mean return of the leng_periods later\n",
    "train_size = int(0.9 * (len(df)-leng_period))  # Example 80% train, 20% test\n",
    "#Using all the micro macro and asset as input\n",
    "X_train = df.iloc[:train_size]\n",
    "X_test = df.iloc[train_size:(len(df)-leng_period)]\n",
    "#using only assets return as output\n",
    "y_train = returns[:train_size]\n",
    "y_test = returns[train_size:(len(df)-leng_period)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ded6d-fd44-4304-84fc-b552ae312f88",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7f97170-77e2-453c-9bb1-cc23a28d5e37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.030461813910110276\n"
     ]
    }
   ],
   "source": [
    "# Standardize features for training and test\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_scaled = scaler.transform(df)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate with Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "prediction = model.predict(X_scaled)\n",
    "prediction = prediction[-leng_period:].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88ae8c69-c3b2-40b6-bd48-802025bb099f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'bootstrap': False, 'max_depth': 26, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 910}\n"
     ]
    }
   ],
   "source": [
    "def random_search_rf(X_train, y_train):\n",
    "    # Define parameter grid\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(100, 1000),\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'max_depth': randint(10, 100),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 20),\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Create Random Forest model\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    # Define mean squared error as scoring metric\n",
    "    mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "    # Perform Randomized Search Cross Validation\n",
    "    random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "                                       n_iter=100, scoring=mse_scorer, cv=5, \n",
    "                                       random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Perform Randomized Search\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    best_rf = random_search.best_estimator_\n",
    "\n",
    "    return best_rf, random_search.best_params_\n",
    "\n",
    "best_rf, best_params = random_search_rf(X_train, y_train)\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb71e1-72c3-4baf-bd8b-1eb5b53e7a84",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50e921c9-b6cf-4085-9a19-d01568133abc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'colsample_bytree': 0.8659851446851979, 'gamma': 0.013808385936852352, 'learning_rate': 0.06788648955075587, 'max_depth': 6, 'min_child_weight': 3, 'n_estimators': 441, 'reg_alpha': 0.32815266747473193, 'reg_lambda': 0.1550416167277442, 'subsample': 0.8945522664931593}\n"
     ]
    }
   ],
   "source": [
    "def random_search_xgb(X_train, y_train):\n",
    "    # Define parameter grid\n",
    "    param_dist = {\n",
    "        'max_depth': randint(3, 7),  \n",
    "        'learning_rate': uniform(0.01, 0.1), \n",
    "        'n_estimators': randint(100, 500),  \n",
    "        'gamma': uniform(0, 0.5), \n",
    "        'min_child_weight': randint(1, 5), \n",
    "        'subsample': uniform(0.6, 0.3),  \n",
    "        'colsample_bytree': uniform(0.6, 0.3),  \n",
    "        'reg_alpha': uniform(0, 1),  \n",
    "        'reg_lambda': uniform(0, 1) \n",
    "    }\n",
    "\n",
    "    # Create XGBoost model\n",
    "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "    # Define mean squared error as scoring metric\n",
    "    mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "    # Perform Randomized Search Cross Validation\n",
    "    random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, \n",
    "                                       n_iter=50, scoring=mse_scorer, cv=5, \n",
    "                                       random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Perform Randomized Search\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    best_xgb = random_search.best_estimator_\n",
    "\n",
    "    return best_xgb, random_search.best_params_\n",
    "\n",
    "best_xgb, best_params = random_search_xgb(X_train, y_train)\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de3a3b-7438-4634-ae28-b4ca1319f5dd",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79296c00-0b51-412f-98f5-299fa698b116",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'estimator__C': 9.64101164904113, 'estimator__gamma': 'scale', 'estimator__kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "def random_search_svm(X_train, y_train):\n",
    "    # Define parameter grid\n",
    "    param_dist = {\n",
    "        'estimator__C': uniform(0.1, 100),\n",
    "        'estimator__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'estimator__gamma': ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "    # Create SVM model\n",
    "    svm_model = SVR()\n",
    "\n",
    "    # Wrap SVM model with MultiOutputRegressor\n",
    "    multioutput_svm = MultiOutputRegressor(svm_model)\n",
    "\n",
    "    # Define mean squared error as scoring metric\n",
    "    mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "    # Perform Randomized Search Cross Validation\n",
    "    random_search = RandomizedSearchCV(estimator=multioutput_svm, param_distributions=param_dist, \n",
    "                                       n_iter=100, scoring=mse_scorer, cv=5, \n",
    "                                       random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Perform Randomized Search\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    best_svm = random_search.best_estimator_\n",
    "\n",
    "    return best_svm, random_search.best_params_\n",
    "\n",
    "best_svm, best_params = random_search_svm(X_train, y_train)\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c24f6-4250-4aea-a5a1-6f00e9dddbcd",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "904fcb33-860f-4ab5-afe9-cda07f0b6cff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'activation': 'tanh', 'alpha': 32.745491628777316, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 289, 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "def random_search_nn(X_train, y_train):\n",
    "    # Define parameter grid\n",
    "    param_dist = {\n",
    "        'hidden_layer_sizes': [(100,), (50, 50), (100, 50, 25)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': np.logspace(-4, 2, 100),\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'max_iter': randint(100, 1000)\n",
    "    }\n",
    "\n",
    "    # Create MLPRegressor model\n",
    "    nn = MLPRegressor(random_state=42)\n",
    "\n",
    "    # Define mean squared error as scoring metric\n",
    "    mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "    # Perform Randomized Search Cross Validation\n",
    "    random_search = RandomizedSearchCV(estimator=nn, param_distributions=param_dist,\n",
    "                                       n_iter=100, scoring=mse_scorer, cv=5,\n",
    "                                       random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Perform Randomized Search\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    best_nn = random_search.best_estimator_\n",
    "\n",
    "    return best_nn, random_search.best_params_\n",
    "\n",
    "# Example usage:\n",
    "best_nn, best_params = random_search_nn(X_train, y_train)\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1d37417-190a-4859-8972-4af8e61f4b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'activation': 'relu', 'dropout_rates': (0.0,), 'hidden_layer_sizes': (100, 50, 25), 'learning_rate': 2.009233002565046, 'max_iter': 800}\n"
     ]
    }
   ],
   "source": [
    "class KerasRegressorWrapper(BaseEstimator):\n",
    "    def __init__(self, hidden_layer_sizes=(100,), activation='relu', dropout_rates=(0.0,), learning_rate=0.001, input_dim=None, max_iter=100):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.dropout_rates = dropout_rates\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_dim = input_dim\n",
    "        self.max_iter = max_iter\n",
    "        self.model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        for i, units in enumerate(self.hidden_layer_sizes):\n",
    "            dropout_rate = self.dropout_rates[min(i, len(self.dropout_rates)-1)]  # Ensure dropout_rates are within bounds\n",
    "            if i == 0:\n",
    "                model.add(Dense(units, activation=self.activation, input_dim=self.input_dim))\n",
    "            else:\n",
    "                model.add(Dense(units, activation=self.activation))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(1))  # Output layer\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        self.model.fit(X_scaled, y, epochs=self.max_iter, verbose=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "\n",
    "def random_search_nn(X_train, y_train):\n",
    "    # Define parameter grid\n",
    "    param_dist = {\n",
    "        'hidden_layer_sizes': [(100,), (50, 50), (100, 50, 25)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'dropout_rates': [(0.0,), (0.1, 0.1), (0.2, 0.2, 0.2)],  # Same length as hidden_layer_sizes\n",
    "        'learning_rate': np.logspace(-4, 2, 100),\n",
    "        'max_iter': randint(100, 1000)\n",
    "    }\n",
    "\n",
    "    # Perform Randomized Search Cross Validation\n",
    "    random_search = RandomizedSearchCV(estimator=KerasRegressorWrapper(), param_distributions=param_dist,\n",
    "                                       n_iter=100, scoring='neg_mean_squared_error', cv=5,\n",
    "                                       random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Perform Randomized Search\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get best parameters\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    # Create and train the best model with the best parameters\n",
    "    best_model = KerasRegressorWrapper(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "\n",
    "best_model, best_params = random_search_nn(X_train, y_train)\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef712664-c024-48cc-ab61-d32ab233af69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
